{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8662c0-506d-4521-be86-7725fcf9217e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Objective: Evaluate saved models on the test set and provide a minimal deployment example.\\nContents:\\n- Load preprocessor and best model\\n- Evaluate on test set (classification report, confusion matrix, ROC-AUC, PR curve)\\n- Threshold tuning\\n- Minimal Flask app example\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 03_testing_and_deployment.ipynb\n",
    "\n",
    "\n",
    "\"\"\"Objective: Evaluate saved models on the test set and provide a minimal deployment example.\n",
    "Contents:\n",
    "- Load preprocessor and best model\n",
    "- Evaluate on test set (classification report, confusion matrix, ROC-AUC, PR curve)\n",
    "- Threshold tuning\n",
    "- Minimal Flask app example\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3744178e-a8fe-4394-87d0-bc6cbc7f9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "\n",
    "PREP_DIR = \"C:\\\\Users\\\\HarshaSri\\\\Desktop\\\\IDS_PROJECT\\\\data\\\\processed\"\n",
    "MODELS_DIR = \"C:\\\\Users\\\\HarshaSri\\\\Desktop\\\\IDS_PROJECT\\\\models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36363eda-6715-4b5f-92bf-17e30dc49e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessor and data\n",
    "preprocessor = joblib.load(os.path.join(PREP_DIR, 'preprocessor.joblib'))\n",
    "X_train_bal, y_train_bal, X_test_t, y_test = joblib.load(os.path.join(PREP_DIR, 'data_splits.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acfde291-5456-446e-b71e-7a4efc098fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models found: ['logistic.joblib', 'rf.joblib', 'rf_best.joblib', 'xgboost.joblib']\n"
     ]
    }
   ],
   "source": [
    "# Find models\n",
    "models = [f for f in os.listdir(MODELS_DIR) if f.endswith('.joblib')]\n",
    "print('Models found:', models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f37d0b80-e868-4fc1-9940-704db8f58510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "logistic.joblib\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9863    0.9900    0.9881      2690\n",
      "           1     0.9885    0.9842    0.9863      2349\n",
      "\n",
      "    accuracy                         0.9873      5039\n",
      "   macro avg     0.9874    0.9871    0.9872      5039\n",
      "weighted avg     0.9873    0.9873    0.9873      5039\n",
      "\n",
      "Confusion matrix:\n",
      " [[2663   27]\n",
      " [  37 2312]]\n",
      "ROC-AUC: 0.9977093788229114\n",
      "\n",
      "\n",
      "rf.joblib\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9952    0.9996    0.9974      2690\n",
      "           1     0.9996    0.9945    0.9970      2349\n",
      "\n",
      "    accuracy                         0.9972      5039\n",
      "   macro avg     0.9974    0.9970    0.9972      5039\n",
      "weighted avg     0.9972    0.9972    0.9972      5039\n",
      "\n",
      "Confusion matrix:\n",
      " [[2689    1]\n",
      " [  13 2336]]\n",
      "ROC-AUC: 0.9999283884149073\n",
      "\n",
      "\n",
      "rf_best.joblib\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9952    0.9989    0.9970      2690\n",
      "           1     0.9987    0.9945    0.9966      2349\n",
      "\n",
      "    accuracy                         0.9968      5039\n",
      "   macro avg     0.9970    0.9967    0.9968      5039\n",
      "weighted avg     0.9968    0.9968    0.9968      5039\n",
      "\n",
      "Confusion matrix:\n",
      " [[2687    3]\n",
      " [  13 2336]]\n",
      "ROC-AUC: 0.9999354308801816\n",
      "\n",
      "\n",
      "xgboost.joblib\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9967    0.9989    0.9978      2690\n",
      "           1     0.9987    0.9962    0.9974      2349\n",
      "\n",
      "    accuracy                         0.9976      5039\n",
      "   macro avg     0.9977    0.9975    0.9976      5039\n",
      "weighted avg     0.9976    0.9976    0.9976      5039\n",
      "\n",
      "Confusion matrix:\n",
      " [[2687    3]\n",
      " [   9 2340]]\n",
      "ROC-AUC: 0.9999441350507452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose a model\n",
    "for i in range(4):\n",
    "    print()\n",
    "    print(models[i])\n",
    "    print()\n",
    "    model_path = os.path.join(MODELS_DIR, models[i]) if models else None\n",
    "    if model_path:\n",
    "        model = joblib.load(model_path)\n",
    "    \n",
    "    # Predict\n",
    "    try:\n",
    "        probs = model.predict_proba(X_test_t)[:,1]\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "    except Exception:\n",
    "        preds = model.predict(X_test_t)\n",
    "        probs = None\n",
    "    # If test labels exist\n",
    "    if y_test is not None:\n",
    "        print('Classification report:')\n",
    "        print(classification_report(y_test, preds, digits=4))\n",
    "        print('Confusion matrix:\\n', confusion_matrix(y_test, preds))\n",
    "        if probs is not None:\n",
    "                print('ROC-AUC:', roc_auc_score(y_test, probs))\n",
    "    else:\n",
    "        print('Test labels not available; outputting predictions for test set')\n",
    "        out = pd.DataFrame({'pred': preds})\n",
    "        if probs is not None:\n",
    "            out['prob'] = probs\n",
    "        out.to_csv(os.path.join(MODELS_DIR, 'test_predictions.csv'), index=False)\n",
    "        print('Saved test_predictions.csv')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff19777-6eb5-4ee7-851f-fb65d93f7997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723de8b7-74ba-42df-b7ef-e0dc2b853c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de15cd-ff25-4362-857d-8e90f9e3130f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3c6db-197b-46ad-b202-fb4c5acff0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
